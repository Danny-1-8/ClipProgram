{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from CLIP import clip\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn as nn\n",
    "device = \"cuda\"\n",
    "\n",
    "from loguru import logger\n",
    "from pytorch_grad_cam import GradCAM, \\\n",
    "                            ScoreCAM, \\\n",
    "                            GradCAMPlusPlus, \\\n",
    "                            AblationCAM, \\\n",
    "                            XGradCAM, \\\n",
    "                            EigenCAM, \\\n",
    "                            EigenGradCAM, \\\n",
    "                            LayerCAM, \\\n",
    "                            FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam import GuidedBackpropReLUModel\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#preprocess for Grad-cam\n",
    "def reshape_transform(tensor, height=7, width=7):\n",
    "    tensor=tensor.reshape(1,tensor.shape[0],tensor.shape[-1])\n",
    "    result = tensor[:, 1:, :].reshape(tensor.size(0),\n",
    "    height, width, tensor.size(2))\n",
    "\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result\n",
    "\n",
    "use_cuda=1\n",
    "finalresult=[]\n",
    " \n",
    "#a wrapper for Grad-cam\n",
    "class load_clip(nn.Module):\n",
    "    def __init__(self,text):\n",
    "        super().__init__()\n",
    "        self.mynet,self.preprocess=clip.load(\"ViT-B/32\", device=device,jit=False)\n",
    "        self.text=text\n",
    "        \n",
    "    def forward(self,image):\n",
    "        logits_per_image, logits_per_text = self.mynet(image, self.text)\n",
    "        probs = logits_per_image.softmax(dim=-1)\n",
    "\n",
    "        return probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line)\n",
    "            data.append(json_obj)\n",
    "    return data\n",
    "\n",
    "text=torch.cat([clip.tokenize(f\"the photo says \\\"gas yourself\\\",this photo is \" + \"hateful\"),\\\n",
    "    clip.tokenize(f\"the photo says \\\"gas yourself\\\",this photo is \" + \"not hateful\")]).to(device)\n",
    "model=load_clip(text)  \n",
    "\n",
    "\n",
    "file_path = '/tmp/DannyWang/CLIP/datasets/hatefulmeme/hateful_memes/dev_seen_m.jsonl'\n",
    "json_data = load_jsonl(file_path)\n",
    "\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "class YourDataset(Dataset):      \n",
    "    def __init__(self,dataset,preprocess,sam_num):\n",
    "\n",
    "        self.img_process = preprocess\n",
    "\n",
    "        self.samples = []\n",
    "        self.sam_labels = []\n",
    "        self.samid=[]\n",
    "        self.caption=[]\n",
    "        catcount=[0,0]\n",
    "\n",
    "        for i in range(len(dataset)):\n",
    "            cl_id=dataset[i]['label']\n",
    "            \n",
    "            if(catcount[cl_id]<sam_num):\n",
    "                if cl_id==0:\n",
    "                    label = f\"this photo is \" + \"not hateful\"\n",
    "                else:\n",
    "                    label = f\"{dataset[i]['text']}\"\n",
    "                self.samples.append(self.img_process(Image.open( '/tmp/DannyWang/CLIP/datasets/hatefulmeme/hateful_memes'+ '/' + dataset[i]['img'])))\n",
    "                self.sam_labels.append(label)\n",
    "                self.samid.append(dataset[i]['label'])\n",
    "                self.caption.append(dataset[i]['text'])\n",
    "                catcount[cl_id]+=1\n",
    "            if(all(item >sam_num-1 for item in catcount)):\n",
    "                break\n",
    "        self.tokens = clip.tokenize(self.sam_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.samples[idx]\n",
    "        token = self.tokens[idx]\n",
    "        caption=self.caption[idx]\n",
    "\n",
    "        \n",
    "        return image,token,self.samid[idx],caption\n",
    "\n",
    "sam_numo=[2,3,4,8]\n",
    "finalresult=[]\n",
    "\n",
    "def mytrain(sam_num):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False)\n",
    "    # 创建损失函数\n",
    "    loss_img = nn.CrossEntropyLoss()\n",
    "    loss_txt = nn.CrossEntropyLoss()\n",
    "    net.visual.requires_grad=False\n",
    "    params = filter(lambda p: p.requires_grad, net.parameters())\n",
    "    optimizer = optim.Adam(params, lr=1e-6,betas=(0.9,0.98),eps=1e-6,weight_decay=0.001)\n",
    "    datasets=YourDataset(json_data,preprocess,sam_num)\n",
    "    your_dataloader=DataLoader(dataset=datasets,batch_size=2,shuffle=False,num_workers=4,pin_memory=False)\n",
    "    total_length=sam_num*2\n",
    "    \n",
    "    phase = \"train\"\n",
    "    epoches = 15\n",
    "    net.train()\n",
    "    \n",
    "    for epoch in range(epoches):\n",
    "        total_loss = 0\n",
    "        batch_num = 0\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            for images,labels,cl_id,caption in your_dataloader:\n",
    "\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    logits_per_image, logits_per_text = net(images, labels)\n",
    "                    ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "                    cur_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "                    total_loss += cur_loss\n",
    "                    if phase == \"train\":\n",
    "                        cur_loss.backward()\n",
    "                        if device == \"cpu\":\n",
    "                            optimizer.step()\n",
    "                        else:\n",
    "                            optimizer.step()\n",
    "                            clip.model.convert_weights(net) \n",
    "                batch_num+=1\n",
    "            epoch_loss = total_loss / total_length\n",
    "            logger.info('{} Loss: {:.4f}'.format(\n",
    "                phase, epoch_loss))\n",
    "\n",
    "            \n",
    "    net.eval()    \n",
    "    image= preprocess(Image.open(\"/tmp/DannyWang/gas.jpg\")).unsqueeze(0).to(device)\n",
    "    image_input = (image)\n",
    "    text_inputs = torch.cat([clip.tokenize(\"this photo is hateful\"),clip.tokenize(\"this photo is not hateful\")]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = net.encode_image(image_input)\n",
    "        text_features = net.encode_text(text_inputs)\n",
    "\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity[0].topk(1)\n",
    "    if indices==0:\n",
    "        print(\"预测对\")\n",
    "    else:\n",
    "        print(\"预测错\")\n",
    "        \n",
    "\n",
    "\n",
    "    model.mynet=net\n",
    "    target_layers=[model.mynet.visual.transformer.resblocks[11].ln_1]       \n",
    "    #the layers where Grad-cam focus on\n",
    "    cam = GradCAM(model=model,\n",
    "            target_layers=target_layers,\n",
    "            use_cuda=use_cuda,\n",
    "            reshape_transform=reshape_transform)\n",
    "\n",
    "    # read the input image\n",
    "    image_path = \"/tmp/DannyWang/gas.jpg\"\n",
    "    rgb_img = cv2.imread(image_path, 1)[:, :, ::-1]\n",
    "    rgb_img = (cv2.resize(rgb_img, (224, 224))).astype(float)\n",
    "\n",
    "    rgb_img/=255\n",
    "\n",
    "    input_tensor = preprocess_image(rgb_img,\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]).to(torch.float32)\n",
    "\n",
    "    if use_cuda:\n",
    "        input_tensor = input_tensor.cuda()\n",
    "\n",
    "    target_category = [ClassifierOutputTarget(1)] #the prediction target that Grad-cam focus on\n",
    "    \n",
    "    #target_category = None\n",
    "    grayscale_cam = cam(input_tensor=(input_tensor), targets=target_category)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "    # lay the output of grad-cam over the raw picture\n",
    "    visualization = show_cam_on_image(rgb_img, grayscale_cam)\n",
    "    cv2.cvtColor(visualization, cv2.COLOR_RGB2BGR, visualization)\n",
    "    cv2.imwrite(f'newftgas15次/opp{sam_num}.jpg', visualization) \n",
    "\n",
    "\n",
    "for i in sam_numo:\n",
    "    mytrain(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "danny",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
